- title: 'Reinforced Continual Learning'
  date: 'Mar. 2018'
  imgurl: '/images/projects/2018/xuju2018_nips.png'
  imgprop: 'frame'
  selected: true
  has_equal_contribution: no
  authors:
    - name: <strong>J. Xu</strong>
      url: 'http://xuju.me'
    - name: Z. Zhu
      url: 'https://sites.google.com/view/zhanxingzhu/'

  publisher: 'NIPS2018'
  place: 'in Canada, Montr√©al'
  desc: 'Most artificial intelligence models have limiting ability to solve new tasks faster, without forgetting previously acquired knowledge. The recently emerging paradigm of continual learning aims to solve this issue, in which the model learns various tasks in a sequential fashion. In this work, a novel approach for continual learning is proposed, which searches for the best neural architecture for each coming task via sophisticatedly designed reinforcement learning strategies. We name it as Reinforced Continual Learning. Our method not only has good performance on preventing catastrophic forgetting but also fits new tasks well. The experiments on sequential classification tasks for variants of MNIST and CIFAR-100 datasets demonstrate that the proposed approach outperforms existing continual learning alternatives for deep networks.'
  tags:
    - name: 'arxiv'
      url: 'https://arxiv.org/pdf/1805.12369.pdf'

- title: 'Bayesian Optimized Continual Learning with Attention Mechanism '
  date: 'Sep. 2018'
  imgurl: '/images/projects/2018/xuju2019_aaai.png'
  imgprop: 'frame'
  selected: true
  has_equal_contribution: no
  authors:
    - name: <strong>J. Xu</strong>
      url: 'http://xuju.me'
    - name: Z. Zhu
      url: 'https://sites.google.com/view/zhanxingzhu/'

  publisher: 'Submitted to AAAI2019'
  place: 'in America, Hawaii'
  desc: 'Though neural networks have achieved much progress in various applications, it is still highly challenging for them to learn from a continuous stream of tasks without forgetting. Continual learning, a new learning paradigm, aims to solve this issue. In this work, we propose a new model for continual learning, called Bayesian Optimized Continual Learning with Attention Mechanism (BOCL) that dynamically expands the network capacity upon the arrival of new tasks by Bayesian optimization and selectively utilizes previous knowledge (e.g. feature maps of previous tasks) via attention mechanism. Our experiments on variants of MNIST and CIFAR-100 demonstrate that our methods outperform the state-of-the-art in preventing catastrophic forgetting and fitting new tasks better.'
  tags:
    - name: no
      url: ''